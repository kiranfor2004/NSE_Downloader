#!/usr/bin/env python3
"""
Step 5 Enhanced: 50% Strike Price Reduction Analysis for ALL SYMBOLS
===================================================================

This enhanced script analyzes ALL symbols in the Step05_strikepriceAnalysisderived 
table to find 50% or more reduction in strike prices by comparing with subsequent
trading data from step04_fo_udiff_daily table.

Enhancements:
- ‚úÖ Process ALL 232 symbols (58,430+ records) instead of limited subset
- ‚úÖ Batch processing for memory efficiency and performance
- ‚úÖ Progress tracking with detailed status updates
- ‚úÖ Resume capability for interrupted processes
- ‚úÖ Comprehensive symbol-wise and market-wide reporting
- ‚úÖ Error handling and retry mechanisms
- ‚úÖ Performance optimization with indexed queries

Author: NSE Data Analysis Team
Date: September 2025
Version: 2.0 (All Symbols Enhancement)
"""

import pyodbc
import pandas as pd
import logging
from datetime import datetime, timedelta
import time
import json
import os

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('step05_50percent_reduction_analysis_all_symbols.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Configuration
BATCH_SIZE = 100  # Process 100 records at a time
PROGRESS_SAVE_INTERVAL = 50  # Save progress every 50 batches
RESUME_FILE = 'step05_analysis_progress.json'

def get_database_connection():
    """Create database connection."""
    connection_string = (
        'Driver={ODBC Driver 17 for SQL Server};'
        'Server=SRIKIRANREDDY\\SQLEXPRESS;'
        'Database=master;'
        'Trusted_Connection=yes;'
    )
    return pyodbc.connect(connection_string)

def create_enhanced_50percent_reduction_table(conn):
    """Create enhanced table to store 50% reduction analysis results for all symbols."""
    
    drop_sql = "IF OBJECT_ID('Step05_50percent_reduction_analysis_all_symbols', 'U') IS NOT NULL DROP TABLE Step05_50percent_reduction_analysis_all_symbols"
    
    create_sql = """
    CREATE TABLE Step05_50percent_reduction_analysis_all_symbols (
        -- Primary key
        reduction_id BIGINT IDENTITY(1,1) PRIMARY KEY,
        
        -- Source data from Step05_strikepriceAnalysisderived
        source_analysis_id BIGINT NOT NULL,
        symbol NVARCHAR(50) NOT NULL,
        base_trade_date DATE NOT NULL,
        strike_price DECIMAL(18,4) NOT NULL,
        option_type VARCHAR(5) NOT NULL,
        base_close_price DECIMAL(18,4) NOT NULL,
        
        -- Reduction analysis results
        reduction_found BIT NOT NULL DEFAULT 0,
        reduction_date DATE NULL,
        reduced_price DECIMAL(18,4) NULL,
        reduction_percentage DECIMAL(10,4) NULL,
        days_to_reduction INT NULL,
        
        -- Additional analysis data
        max_reduction_percentage DECIMAL(10,4) NULL,
        max_reduction_date DATE NULL,
        max_reduction_price DECIMAL(18,4) NULL,
        total_trading_days_analyzed INT NULL,
        
        -- Performance metrics
        price_volatility DECIMAL(10,4) NULL,
        avg_daily_reduction DECIMAL(10,4) NULL,
        
        -- Batch processing metadata
        batch_number INT NULL,
        processing_timestamp DATETIME2 DEFAULT GETDATE(),
        
        -- Enhanced indexes for performance
        INDEX IX_all_symbols_symbol (symbol),
        INDEX IX_all_symbols_strike (strike_price),
        INDEX IX_all_symbols_option (option_type),
        INDEX IX_all_symbols_reduction_found (reduction_found),
        INDEX IX_all_symbols_batch (batch_number),
        INDEX IX_all_symbols_symbol_strike (symbol, strike_price),
        INDEX IX_all_symbols_reduction_perf (reduction_found, reduction_percentage DESC)
    )
    """
    
    cursor = conn.cursor()
    
    try:
        logger.info("Dropping existing Step05_50percent_reduction_analysis_all_symbols table if exists...")
        cursor.execute(drop_sql)
        
        logger.info("Creating Step05_50percent_reduction_analysis_all_symbols table...")
        cursor.execute(create_sql)
        
        conn.commit()
        logger.info("‚úÖ Step05_50percent_reduction_analysis_all_symbols table created successfully")
        
    except Exception as e:
        logger.error(f"Error creating table: {e}")
        conn.rollback()
        raise
    finally:
        cursor.close()

def get_all_step05_base_data_batched(conn, batch_size=BATCH_SIZE, offset=0):
    """Get all records from Step05_strikepriceAnalysisderived in batches."""
    logger.info(f"Getting base data batch (offset: {offset}, size: {batch_size})...")
    
    query = """
    SELECT 
        analysis_id,
        Symbol,
        Current_trade_date,
        Strike_price,
        option_type,
        close_price as base_close_price
    FROM Step05_strikepriceAnalysisderived
    ORDER BY Symbol, Strike_price, option_type
    OFFSET ? ROWS FETCH NEXT ? ROWS ONLY
    """
    
    df = pd.read_sql(query, conn, params=[offset, batch_size])
    logger.info(f"Retrieved {len(df)} records in batch")
    
    return df

def get_total_base_records(conn):
    """Get total number of records to process."""
    cursor = conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM Step05_strikepriceAnalysisderived")
    total = cursor.fetchone()[0]
    cursor.close()
    return total

def get_subsequent_trading_data_optimized(conn, symbol, strike_price, option_type, start_date):
    """
    Optimized version to get subsequent trading data using indexed query.
    """
    # Convert start_date to YYYYMMDD format for comparison
    if isinstance(start_date, str):
        from datetime import datetime
        date_obj = datetime.strptime(start_date, '%Y-%m-%d')
        next_day = date_obj + timedelta(days=1)
        start_date_formatted = next_day.strftime('%Y%m%d')
    else:
        next_day = start_date + timedelta(days=1)
        start_date_formatted = next_day.strftime('%Y%m%d')
    
    # Optimized query with proper indexing
    query = """
    SELECT 
        trade_date,
        close_price
    FROM step04_fo_udiff_daily WITH (INDEX(IX_step04_symbol))
    WHERE symbol = ?
    AND strike_price = ?
    AND option_type = ?
    AND trade_date >= ?
    AND close_price IS NOT NULL
    ORDER BY trade_date
    """
    
    try:
        df = pd.read_sql(query, conn, params=[symbol, strike_price, option_type, start_date_formatted])
        return df
    except Exception as e:
        logger.warning(f"Error getting subsequent data for {symbol} {strike_price} {option_type}: {e}")
        return pd.DataFrame()

def analyze_50percent_reduction_enhanced(base_price, subsequent_data):
    """
    Enhanced analysis with additional metrics.
    """
    if subsequent_data.empty:
        return {
            'reduction_found': False,
            'reduction_date': None,
            'reduced_price': None,
            'reduction_percentage': None,
            'days_to_reduction': None,
            'max_reduction_percentage': None,
            'max_reduction_date': None,
            'max_reduction_price': None,
            'total_trading_days_analyzed': 0,
            'price_volatility': None,
            'avg_daily_reduction': None
        }
    
    # Calculate reduction percentages for all subsequent prices
    subsequent_data = subsequent_data.copy()
    subsequent_data['reduction_percentage'] = ((base_price - subsequent_data['close_price']) / base_price) * 100
    
    # Calculate additional metrics
    price_volatility = subsequent_data['close_price'].std() / subsequent_data['close_price'].mean() * 100 if len(subsequent_data) > 1 else 0
    avg_daily_reduction = subsequent_data['reduction_percentage'].mean()
    
    # Find first occurrence of 50% or more reduction
    fifty_percent_reductions = subsequent_data[subsequent_data['reduction_percentage'] >= 50.0]
    
    # Find maximum reduction
    max_reduction_row = subsequent_data.loc[subsequent_data['reduction_percentage'].idxmax()]
    
    result = {
        'total_trading_days_analyzed': len(subsequent_data),
        'max_reduction_percentage': max_reduction_row['reduction_percentage'],
        'max_reduction_date': max_reduction_row['trade_date'],
        'max_reduction_price': max_reduction_row['close_price'],
        'price_volatility': price_volatility,
        'avg_daily_reduction': avg_daily_reduction
    }
    
    if not fifty_percent_reductions.empty:
        # 50% reduction found
        first_50_percent = fifty_percent_reductions.iloc[0]
        
        # Calculate days to reduction
        base_date = datetime.strptime(subsequent_data.iloc[0]['trade_date'], '%Y%m%d') - timedelta(days=1)
        reduction_date = datetime.strptime(first_50_percent['trade_date'], '%Y%m%d')
        days_to_reduction = (reduction_date - base_date).days
        
        result.update({
            'reduction_found': True,
            'reduction_date': first_50_percent['trade_date'],
            'reduced_price': first_50_percent['close_price'],
            'reduction_percentage': first_50_percent['reduction_percentage'],
            'days_to_reduction': days_to_reduction
        })
        
    else:
        # No 50% reduction found
        result.update({
            'reduction_found': False,
            'reduction_date': None,
            'reduced_price': None,
            'reduction_percentage': None,
            'days_to_reduction': None
        })
    
    return result

def process_batch(conn, batch_data, batch_number):
    """Process a single batch of records."""
    batch_results = []
    
    for index, base_record in batch_data.iterrows():
        try:
            symbol = base_record['Symbol']
            strike_price = base_record['Strike_price']
            option_type = base_record['option_type']
            base_date = base_record['Current_trade_date']
            base_price = base_record['base_close_price']
            
            # Get subsequent trading data
            subsequent_data = get_subsequent_trading_data_optimized(conn, symbol, strike_price, option_type, base_date)
            
            # Analyze for 50% reduction
            analysis_result = analyze_50percent_reduction_enhanced(base_price, subsequent_data)
            
            # Prepare data for database insertion
            result = {
                'source_analysis_id': base_record['analysis_id'],
                'symbol': symbol,
                'base_trade_date': base_date,
                'strike_price': strike_price,
                'option_type': option_type,
                'base_close_price': base_price,
                'batch_number': batch_number,
                **analysis_result
            }
            
            batch_results.append(result)
            
        except Exception as e:
            logger.error(f"Error processing record {index} in batch {batch_number}: {e}")
            continue
    
    return batch_results

def insert_batch_results(conn, batch_results):
    """Insert batch results into the database."""
    
    if not batch_results:
        return 0
    
    insert_sql = """
    INSERT INTO Step05_50percent_reduction_analysis_all_symbols (
        source_analysis_id, symbol, base_trade_date, strike_price, option_type, base_close_price,
        reduction_found, reduction_date, reduced_price, reduction_percentage, days_to_reduction,
        max_reduction_percentage, max_reduction_date, max_reduction_price, total_trading_days_analyzed,
        price_volatility, avg_daily_reduction, batch_number
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """
    
    cursor = conn.cursor()
    inserted_count = 0
    
    try:
        for result in batch_results:
            values = (
                result['source_analysis_id'],
                result['symbol'],
                result['base_trade_date'],
                result['strike_price'],
                result['option_type'],
                result['base_close_price'],
                result['reduction_found'],
                result['reduction_date'],
                result['reduced_price'],
                result['reduction_percentage'],
                result['days_to_reduction'],
                result['max_reduction_percentage'],
                result['max_reduction_date'],
                result['max_reduction_price'],
                result['total_trading_days_analyzed'],
                result['price_volatility'],
                result['avg_daily_reduction'],
                result['batch_number']
            )
            
            cursor.execute(insert_sql, values)
            inserted_count += 1
        
        conn.commit()
        
    except Exception as e:
        logger.error(f"Error inserting batch results: {e}")
        conn.rollback()
        raise
    finally:
        cursor.close()
    
    return inserted_count

def save_progress(batch_number, total_batches, processed_records):
    """Save progress to resume file."""
    progress = {
        'batch_number': batch_number,
        'total_batches': total_batches,
        'processed_records': processed_records,
        'timestamp': datetime.now().isoformat()
    }
    
    with open(RESUME_FILE, 'w') as f:
        json.dump(progress, f)

def load_progress():
    """Load progress from resume file."""
    if os.path.exists(RESUME_FILE):
        try:
            with open(RESUME_FILE, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"Could not load progress file: {e}")
    
    return None

def generate_comprehensive_all_symbols_report(conn):
    """Generate comprehensive report for all symbols analysis."""
    logger.info("Generating comprehensive all-symbols analysis report...")
    
    cursor = conn.cursor()
    
    print("\n" + "="*120)
    print("STEP 5 ENHANCED: 50% STRIKE PRICE REDUCTION ANALYSIS - ALL SYMBOLS COMPREHENSIVE REPORT")
    print("="*120)
    
    # Overall statistics
    cursor.execute("""
    SELECT 
        COUNT(*) as total_strikes_analyzed,
        COUNT(DISTINCT symbol) as unique_symbols,
        COUNT(CASE WHEN reduction_found = 1 THEN 1 END) as strikes_with_50percent_reduction,
        COUNT(CASE WHEN reduction_found = 0 THEN 1 END) as strikes_without_50percent_reduction,
        AVG(CASE WHEN reduction_found = 1 THEN reduction_percentage END) as avg_reduction_percentage,
        AVG(CASE WHEN reduction_found = 1 THEN days_to_reduction END) as avg_days_to_reduction,
        MIN(CASE WHEN reduction_found = 1 THEN days_to_reduction END) as min_days_to_reduction,
        MAX(CASE WHEN reduction_found = 1 THEN days_to_reduction END) as max_days_to_reduction,
        AVG(price_volatility) as avg_price_volatility,
        AVG(max_reduction_percentage) as avg_max_reduction
    FROM Step05_50percent_reduction_analysis_all_symbols
    """)
    
    overall_stats = cursor.fetchone()
    
    print(f"üìä OVERALL MARKET STATISTICS:")
    print(f"Total strikes analyzed: {overall_stats[0]:,}")
    print(f"Unique symbols covered: {overall_stats[1]:,}")
    print(f"Strikes with 50%+ reduction: {overall_stats[2]:,}")
    print(f"Strikes without 50%+ reduction: {overall_stats[3]:,}")
    
    if overall_stats[2] > 0:
        success_rate = (overall_stats[2] / overall_stats[0]) * 100
        print(f"Overall success rate: {success_rate:.2f}%")
        print(f"Average reduction percentage: {overall_stats[4]:.2f}%")
        print(f"Average days to 50% reduction: {overall_stats[5]:.1f} days")
        print(f"Fastest 50% reduction: {overall_stats[6]} days")
        print(f"Slowest 50% reduction: {overall_stats[7]} days")
        print(f"Average price volatility: {overall_stats[8]:.2f}%")
        print(f"Average maximum reduction: {overall_stats[9]:.2f}%")
    
    # Symbol-wise performance
    print(f"\nüèÜ TOP 20 SYMBOLS BY 50% REDUCTION SUCCESS RATE:")
    cursor.execute("""
    SELECT TOP 20
        symbol,
        COUNT(*) as total_strikes,
        COUNT(CASE WHEN reduction_found = 1 THEN 1 END) as successful_reductions,
        CAST(COUNT(CASE WHEN reduction_found = 1 THEN 1 END) * 100.0 / COUNT(*) AS DECIMAL(5,2)) as success_rate,
        AVG(CASE WHEN reduction_found = 1 THEN reduction_percentage END) as avg_reduction_pct,
        AVG(CASE WHEN reduction_found = 1 THEN days_to_reduction END) as avg_days
    FROM Step05_50percent_reduction_analysis_all_symbols
    GROUP BY symbol
    HAVING COUNT(*) >= 10  -- At least 10 strikes for meaningful analysis
    ORDER BY success_rate DESC, successful_reductions DESC
    """)
    
    print(f"{'Symbol':<12} {'Strikes':<8} {'Success':<8} {'Rate%':<8} {'Avg%':<8} {'Days':<6}")
    print("-" * 60)
    
    for row in cursor.fetchall():
        symbol, total, success, rate, avg_pct, avg_days = row
        avg_pct_str = f"{avg_pct:.1f}" if avg_pct else "N/A"
        avg_days_str = f"{avg_days:.1f}" if avg_days else "N/A"
        print(f"{symbol:<12} {total:<8} {success:<8} {rate:<8} {avg_pct_str:<8} {avg_days_str:<6}")
    
    # Worst performers
    print(f"\nüîª SYMBOLS WITH LOWEST SUCCESS RATES (Min 10 strikes):")
    cursor.execute("""
    SELECT TOP 10
        symbol,
        COUNT(*) as total_strikes,
        COUNT(CASE WHEN reduction_found = 1 THEN 1 END) as successful_reductions,
        CAST(COUNT(CASE WHEN reduction_found = 1 THEN 1 END) * 100.0 / COUNT(*) AS DECIMAL(5,2)) as success_rate,
        AVG(max_reduction_percentage) as avg_max_reduction
    FROM Step05_50percent_reduction_analysis_all_symbols
    GROUP BY symbol
    HAVING COUNT(*) >= 10
    ORDER BY success_rate ASC
    """)
    
    print(f"{'Symbol':<12} {'Strikes':<8} {'Success':<8} {'Rate%':<8} {'MaxRed%':<8}")
    print("-" * 50)
    
    for row in cursor.fetchall():
        symbol, total, success, rate, max_red = row
        max_red_str = f"{max_red:.1f}" if max_red else "N/A"
        print(f"{symbol:<12} {total:<8} {success:<8} {rate:<8} {max_red_str:<8}")
    
    # Strike price analysis
    print(f"\nüìà STRIKE PRICE RANGE ANALYSIS:")
    cursor.execute("""
    SELECT 
        CASE 
            WHEN strike_price < 100 THEN 'Under ‚Çπ100'
            WHEN strike_price BETWEEN 100 AND 500 THEN '‚Çπ100-500'
            WHEN strike_price BETWEEN 500 AND 1000 THEN '‚Çπ500-1000'
            WHEN strike_price BETWEEN 1000 AND 5000 THEN '‚Çπ1000-5000'
            ELSE 'Above ‚Çπ5000'
        END as strike_range,
        COUNT(*) as total_strikes,
        COUNT(CASE WHEN reduction_found = 1 THEN 1 END) as successful_reductions,
        CAST(COUNT(CASE WHEN reduction_found = 1 THEN 1 END) * 100.0 / COUNT(*) AS DECIMAL(5,2)) as success_rate
    FROM Step05_50percent_reduction_analysis_all_symbols
    GROUP BY 
        CASE 
            WHEN strike_price < 100 THEN 'Under ‚Çπ100'
            WHEN strike_price BETWEEN 100 AND 500 THEN '‚Çπ100-500'
            WHEN strike_price BETWEEN 500 AND 1000 THEN '‚Çπ500-1000'
            WHEN strike_price BETWEEN 1000 AND 5000 THEN '‚Çπ1000-5000'
            ELSE 'Above ‚Çπ5000'
        END
    ORDER BY success_rate DESC
    """)
    
    for row in cursor.fetchall():
        range_name, total, success, rate = row
        print(f"   {range_name:<12}: {success:,}/{total:,} strikes ({rate}% success rate)")
    
    # Option type analysis
    print(f"\nüìä OPTION TYPE ANALYSIS:")
    cursor.execute("""
    SELECT 
        option_type,
        COUNT(*) as total_strikes,
        COUNT(CASE WHEN reduction_found = 1 THEN 1 END) as successful_reductions,
        CAST(COUNT(CASE WHEN reduction_found = 1 THEN 1 END) * 100.0 / COUNT(*) AS DECIMAL(5,2)) as success_rate,
        AVG(CASE WHEN reduction_found = 1 THEN reduction_percentage END) as avg_reduction_pct
    FROM Step05_50percent_reduction_analysis_all_symbols
    GROUP BY option_type
    ORDER BY success_rate DESC
    """)
    
    for row in cursor.fetchall():
        option_type, total, success, rate, avg_pct = row
        avg_pct_str = f"{avg_pct:.1f}%" if avg_pct else "N/A"
        print(f"   {option_type}: {success:,}/{total:,} strikes ({rate}% success rate, avg {avg_pct_str})")
    
    print("="*120)
    cursor.close()

def main():
    """Main function to execute enhanced all-symbols 50% reduction analysis."""
    print("üéØ STEP 5 ENHANCED: 50% STRIKE PRICE REDUCTION ANALYSIS - ALL SYMBOLS")
    print("="*80)
    print("Processing ALL symbols from Step05_strikepriceAnalysisderived with enhanced analytics")
    print("Batch processing enabled for optimal performance and memory management")
    print("="*80)
    
    start_time = time.time()
    
    try:
        conn = get_database_connection()
        logger.info("Database connection established")
        
        # Check for resume capability
        progress = load_progress()
        start_batch = 0
        processed_records = 0
        
        if progress:
            print(f"\nüîÑ Resume option available from previous run:")
            print(f"   Last batch: {progress['batch_number']}/{progress['total_batches']}")
            print(f"   Records processed: {progress['processed_records']:,}")
            print(f"   Timestamp: {progress['timestamp']}")
            
            resume = input("\nResume from previous progress? (y/n): ").lower().strip()
            if resume == 'y':
                start_batch = progress['batch_number']
                processed_records = progress['processed_records']
                print(f"‚úÖ Resuming from batch {start_batch}")
            else:
                # Create fresh table
                create_enhanced_50percent_reduction_table(conn)
        else:
            # Create the reduction analysis table
            create_enhanced_50percent_reduction_table(conn)
        
        # Get total records
        total_records = get_total_base_records(conn)
        total_batches = (total_records + BATCH_SIZE - 1) // BATCH_SIZE
        
        print(f"\nüìä PROCESSING SCOPE:")
        print(f"   Total records: {total_records:,}")
        print(f"   Batch size: {BATCH_SIZE}")
        print(f"   Total batches: {total_batches}")
        print(f"   Starting from batch: {start_batch + 1}")
        
        # Process batches
        for batch_num in range(start_batch, total_batches):
            batch_start_time = time.time()
            offset = batch_num * BATCH_SIZE
            
            print(f"\nüîÑ Processing Batch {batch_num + 1}/{total_batches}")
            print(f"   Offset: {offset:,} | Records: {min(BATCH_SIZE, total_records - offset)}")
            
            # Get batch data
            batch_data = get_all_step05_base_data_batched(conn, BATCH_SIZE, offset)
            
            if batch_data.empty:
                print(f"   ‚ö†Ô∏è No data in batch {batch_num + 1}, skipping...")
                continue
            
            # Process batch
            batch_results = process_batch(conn, batch_data, batch_num + 1)
            
            # Insert results
            inserted_count = insert_batch_results(conn, batch_results)
            processed_records += inserted_count
            
            batch_time = time.time() - batch_start_time
            progress_pct = ((batch_num + 1) / total_batches) * 100
            
            print(f"   ‚úÖ Batch {batch_num + 1} completed: {inserted_count} records inserted")
            print(f"   ‚è±Ô∏è Batch time: {batch_time:.2f}s | Progress: {progress_pct:.1f}%")
            
            # Save progress periodically
            if (batch_num + 1) % PROGRESS_SAVE_INTERVAL == 0:
                save_progress(batch_num + 1, total_batches, processed_records)
                print(f"   üíæ Progress saved")
            
            # Estimate remaining time
            if batch_num > start_batch:
                avg_batch_time = (time.time() - start_time) / (batch_num - start_batch + 1)
                remaining_batches = total_batches - batch_num - 1
                eta_seconds = remaining_batches * avg_batch_time
                eta_minutes = eta_seconds / 60
                print(f"   üïê ETA: {eta_minutes:.1f} minutes")
        
        # Clean up progress file
        if os.path.exists(RESUME_FILE):
            os.remove(RESUME_FILE)
        
        total_time = time.time() - start_time
        
        print(f"\nüéØ ALL-SYMBOLS ANALYSIS COMPLETED!")
        print(f"Records processed: {total_records:,}")
        print(f"Total time: {total_time/60:.1f} minutes")
        print(f"Average: {total_records/(total_time/60):.0f} records/minute")
        
        # Generate comprehensive report
        generate_comprehensive_all_symbols_report(conn)
        
        print(f"\n‚úÖ Enhanced 50% reduction analysis completed successfully!")
        print(f"üìã Results stored in Step05_50percent_reduction_analysis_all_symbols table")
        
    except Exception as e:
        logger.error(f"All-symbols analysis error: {e}")
        print(f"‚ùå Error: {e}")
        
        # Save progress on error
        if 'batch_num' in locals():
            save_progress(batch_num, total_batches, processed_records)
            print(f"üíæ Progress saved for resume capability")
    
    finally:
        if 'conn' in locals():
            conn.close()
            logger.info("Database connection closed")

if __name__ == "__main__":
    main()